import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Sample data
X1 = np.array([1.0, 2.0, 2.5, 3.0, 3.5, 4.0]).reshape(-1, 1)  # Feature 1
X2 = np.array([2.0, 1.5, 2.0, 3.0, 3.5, 4.0]).reshape(-1, 1)  # Feature 2
y = np.array([2.5, 3.0, 4.5, 5.0, 6.5, 7.0])  # Target

# Linear Regression using only X1
lin_reg = LinearRegression()
lin_reg.fit(X1, y)

y_pred_lin = lin_reg.predict(X1)
print("Linear Regression Model:")
print(f"Equation: y = {lin_reg.intercept_:.2f} + {lin_reg.coef_[0]:.2f}*X1")
print(f"MSE: {mean_squared_error(y, y_pred_lin):.4f}")
print(f"R² Score: {r2_score(y, y_pred_lin):.4f}\n")

# Multiple Linear Regression using X1 and X2
X_multi = np.hstack((X1, X2))
multi_reg = LinearRegression()
multi_reg.fit(X_multi, y)

y_pred_multi = multi_reg.predict(X_multi)
print("Multiple Linear Regression Model:")
print(f"Equation: y = {multi_reg.intercept_:.2f} + {multi_reg.coef_[0]:.2f}*X1 + {multi_reg.coef_[1]:.2f}*X2")
print(f"MSE: {mean_squared_error(y, y_pred_multi):.4f}")
print(f"R² Score: {r2_score(y, y_pred_multi):.4f}")

# Predictions
new_x1, new_x2 = 4.0, 3.0
y_pred_new_lin = lin_reg.predict(np.array([[new_x1]]))[0]
y_pred_new_multi = multi_reg.predict(np.array([[new_x1, new_x2]]))[0]
print(f"Prediction for X1={new_x1}: y = {y_pred_new_lin:.2f} (Linear Regression)")
print(f"Prediction for X1={new_x1}, X2={new_x2}: y = {y_pred_new_multi:.2f} (Multiple Linear Regression)")

# Visualization
plt.scatter(X1, y, color='blue', label='Actual Data')
plt.plot(X1, y_pred_lin, color='red', linestyle='dashed', label='Linear Regression')
plt.xlabel("Feature 1 (X1)")
plt.ylabel("Target (y)")
plt.title("Linear Regression vs Actual Data")
plt.legend()
plt.show()
